import os, json, time
import streamlit as st

# ì—ì´ì „íŠ¸ ëª¨ë“ˆ
from agents.news_crawler import NewsCrawlerAgent
from agents.news_ranker import NewsRankerAgent
from agents.news_analyst import NewsAnalystAgent
from agents.pdf_rag import ingest_pdfs, query_pdf_knowledge, collection  # collection.count() ì‚¬ìš©
from agents.portfolio_agent import PortfolioAgent
from agents.orchestrator import OrchestratorAgent
from agents.econ_reporter import EconReporterAgent



st.set_page_config(page_title="ê²½ì œ ë‰´ìŠ¤ ì—ì´ì „íŠ¸ (All-in-One)", layout="wide")
st.title("ğŸ§  ê²½ì œ ë‰´ìŠ¤ ì—ì´ì „íŠ¸ ")

# ---------------------------
# ì‚¬ì´ë“œë°”: ì„¤ì •
# ---------------------------
st.sidebar.header("âš™ï¸ ì„¤ì •")

# ë‰´ìŠ¤ ì„¤ì •
news_count = st.sidebar.slider("ë‰´ìŠ¤ ê°œìˆ˜ (Top N)", min_value=3, max_value=15, value=6, step=1)
horizon_hours = st.sidebar.slider("ìµœê·¼ Nì‹œê°„ ê¸°ì‚¬ë§Œ ë³´ê¸°", min_value=6, max_value=48, value=18, step=3)

# í¬íŠ¸í´ë¦¬ì˜¤ ì„¤ì • (portfolio.json ë¡œë“œ/ì €ì¥)
st.sidebar.subheader("ğŸ“¦ ë‚´ í¬íŠ¸í´ë¦¬ì˜¤")
default_tickers = ["AAPL", "TSLA", "MSFT"]
existing = default_tickers
if os.path.exists("portfolio.json"):
    try:
        with open("portfolio.json", "r", encoding="utf-8") as f:
            data = json.load(f)
            existing = data.get("tickers", default_tickers)
    except Exception:
        existing = default_tickers
seed = ",".join(existing)
user_input = st.sidebar.text_input("ì¢…ëª© ì½”ë“œ(ì½¤ë§ˆë¡œ êµ¬ë¶„, ì˜ˆ: AAPL,TSLA,005930.KS)", seed)
if st.sidebar.button("í¬íŠ¸í´ë¦¬ì˜¤ ì €ì¥"):
    tickers = [t.strip() for t in user_input.split(",") if t.strip()]
    with open("portfolio.json", "w", encoding="utf-8") as f:
        json.dump({"tickers": tickers}, f, ensure_ascii=False, indent=2)
    st.sidebar.success("ì €ì¥ ì™„ë£Œ! í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨(F5) ì‹œ ë°˜ì˜ë©ë‹ˆë‹¤.")

# PDF RAG ì¸ë±ì‹± ì‹¤í–‰
st.sidebar.subheader("ğŸ“š PDF RAG")
if st.sidebar.button("data/pdfs í´ë” ì¸ë±ì‹± ì‹¤í–‰"):
    with st.spinner("PDF ì¸ë±ì‹± ì¤‘... (ìš©ëŸ‰/í˜ì´ì§€ ìˆ˜ì— ë”°ë¼ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤)"):
        ingest_pdfs()
    st.sidebar.success("PDF ì¸ë±ì‹± ì™„ë£Œ!")

# RAG ë¹ ë¥¸ ê²€ìƒ‰
quick_query = st.sidebar.text_input("RAG ë¹ ë¥¸ ê²€ìƒ‰ (ì˜ˆ: ì¸í”Œë ˆì´ì…˜)")
if st.sidebar.button("ê²€ìƒ‰"):
    r = query_pdf_knowledge(quick_query, n_results=3)
    st.sidebar.write("ê²€ìƒ‰ ê²°ê³¼:")
    if not r:
        st.sidebar.info("ê´€ë ¨ ë¬¸ì„œ ì—†ìŒ")
    else:
        for i, item in enumerate(r, 1) if isinstance(r[0], dict) else enumerate(r, 1):
            if isinstance(item, dict):
                st.sidebar.markdown(f"**[{i}] {item.get('source','unknown')}**\n\n{item.get('content','')[:200]}â€¦")
            else:
                st.sidebar.markdown(f"**[{i}]** {item[:200]}â€¦")

# ---------------------------
# íƒ­ êµ¬ì„±
# ---------------------------
tab_news, tab_rag, tab_port, tab_signal, tab_status = st.tabs(
    ["ğŸ“° ë‰´ìŠ¤/ë¶„ì„", "ğŸ” RAG ê²€ìƒ‰", "ğŸ“ˆ í¬íŠ¸í´ë¦¬ì˜¤", "ğŸš€ ì¢…ëª© ì‹œê·¸ë„", "ğŸ›  ìƒíƒœ/ì„¤ì •"]
)

# ì „ì—­ ìƒíƒœ ìºì‹œ
if "latest_news" not in st.session_state:
    st.session_state.latest_news = []     # í¬ë¡¤ë§ ì›ë³¸
if "ranked_news" not in st.session_state:
    st.session_state.ranked_news = []     # (score, article)
if "analyzed" not in st.session_state:
    st.session_state.analyzed = []        # ë¶„ì„ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸

# ---------------------------
# íƒ­ 1: ë‰´ìŠ¤/ë¶„ì„
# ---------------------------
with tab_news:
    st.subheader("ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤/ë¸”ë¡œê·¸ â†’ ë­í‚¹ â†’ ë¶„ì„(+RAG)")

    # ğŸ”½ ì¶”ê°€: ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ
    mode = st.selectbox(
        "ë°ì´í„° ì†ŒìŠ¤ ì„ íƒ",
        ["news", "blog", "both"],
        format_func=lambda x: {"news":"ë‰´ìŠ¤", "blog":"ë¸”ë¡œê·¸", "both":"ë‘˜ ë‹¤"}[x]
    )

    col1, col2, col3 = st.columns(3)
    with col1:
        do_fetch = st.button("1) ë‰´ìŠ¤/ë¸”ë¡œê·¸ í¬ë¡¤ë§")
    with col2:
        do_analyze = st.button("2) ìƒìœ„ N ë¶„ì„ ì‹¤í–‰")
    with col3:
        do_run_all = st.button("ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")

    # ğŸš€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    if do_run_all:
        tickers = [t.strip() for t in user_input.split(",") if t.strip()]
        orch = OrchestratorAgent(tickers=tickers, topk=news_count, horizon_hours=horizon_hours)
        with st.spinner(f"{mode} íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘..."):
            res = orch.run(source=mode)
        st.session_state.latest_news = res["articles"]
        st.session_state.ranked_news = res["ranked"]
        st.session_state.analyzed = res["analyzed"]
        st.session_state.portfolio_prices = res.get("portfolio_prices", {})
        st.success(f"âœ… ì™„ë£Œ! ({mode}) ìˆ˜ì§‘ {len(res['articles'])}ê°œ / ë¶„ì„ {len(res['analyzed'])}ê±´")

    # 1) í¬ë¡¤ë§ + ë­í‚¹ (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
    if do_fetch:
        with st.spinner("ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘..."):
            crawler = NewsCrawlerAgent(horizon_hours=horizon_hours)
            articles = crawler.collect_items()
            st.session_state.latest_news = articles
        st.success(f"í¬ë¡¤ë§ ì™„ë£Œ: ì´ {len(st.session_state.latest_news)}ê°œ ê¸°ì‚¬")

        with st.spinner("ë­í‚¹/ì¤‘ë³µì œê±° ì¤‘..."):
            ranker = NewsRankerAgent(topk=news_count)
            ranked = ranker.rank_items(st.session_state.latest_news)
            st.session_state.ranked_news = ranked
        st.info(f"ìƒìœ„ {len(st.session_state.ranked_news)}ê°œ ê¸°ì‚¬ ì„ ì •")

    # ë­í‚¹ ë¯¸ë¦¬ë³´ê¸° (ê¸°ì¡´)
    if st.session_state.ranked_news:
        st.markdown("**ìƒìœ„ ê¸°ì‚¬ ë¯¸ë¦¬ë³´ê¸°**")
        for i, (score, art) in enumerate(st.session_state.ranked_news, 1):
            with st.expander(f"[{i}] {art['title']}  Â·  ì ìˆ˜ {round(score,2)}  Â·  {art['published']}"):
                st.markdown(f"- ë§í¬: {art['link']}")
                st.markdown(f"- ìš”ì•½(ì›ë¬¸ ì œê³µ): {art['summary'][:300]}â€¦")

    # 2) Analyst(+RAG) ì‹¤í–‰ (ê¸°ì¡´)
    if do_analyze:
        if not st.session_state.ranked_news:
            st.warning("ë¨¼ì € 'ë‰´ìŠ¤ í¬ë¡¤ë§'ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
        else:
            st.session_state.analyzed = []
            analyst = NewsAnalystAgent()
            with st.spinner("Analystê°€ ìƒìœ„ ê¸°ì‚¬ ë¶„ì„ ì¤‘..."):
                for _, art in st.session_state.ranked_news:
                    result = analyst.analyze(art)
                    st.session_state.analyzed.append({"article": art, "analysis": result})
            st.success("ë¶„ì„ ì™„ë£Œ!")

    # ë¶„ì„ ê²°ê³¼ í‘œì‹œ (ê¸°ì¡´)
    if st.session_state.analyzed:
        st.markdown("## ê²°ê³¼")
        for i, item in enumerate(st.session_state.analyzed, 1):
            art = item["article"]
            res = item["analysis"]
            with st.expander(f"{i}. {res.get('headline', art['title'])}  Â·  {art['published']}"):
                st.write(f"[ì›ë¬¸ ë³´ê¸°]({art['link']})")
                st.markdown(f"**ìš”ì•½:** {res.get('summary','')}")
                st.markdown(f"**ì˜í–¥:** {res.get('impact','')}")
                kws = res.get("keywords", [])
                if kws:
                    st.markdown(f"**í‚¤ì›Œë“œ:** {', '.join(kws)}")
                rag_ctx = res.get("rag_context", [])
                if rag_ctx:
                    st.markdown("**ğŸ“š ì¶”ê°€ ì§€ì‹ (PDF RAG):**")
                    for ctx in rag_ctx:
                        st.markdown(f"- {ctx}")
        st.markdown("---")
        if st.button("ğŸ“ ì˜¤ëŠ˜ ë¦¬í¬íŠ¸(MD) ìƒì„±/ì €ì¥"):
            rep = EconReporterAgent()
            md = rep.build_report(st.session_state.analyzed)
            path = rep.save_report(md)
            st.success(f"ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {path}")

            # ë°”ë¡œ í™”ë©´ì—ì„œ ë³´ê¸°
            with st.expander("ë¦¬í¬íŠ¸ ë¯¸ë¦¬ë³´ê¸° (Markdown)"):
                st.markdown(md)
# ---------------------------
# íƒ­ 2: RAG ê²€ìƒ‰ (ì¸í„°ë™í‹°ë¸Œ)
# ---------------------------
with tab_rag:
    st.subheader("PDF RAG ê²€ìƒ‰")
    q = st.text_input("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ì¸í”Œë ˆì´ì…˜, ê¸ˆë¦¬, GDP ë“±)", value=quick_query or "")
    nres = st.slider("ê²€ìƒ‰ ê²°ê³¼ ê°œìˆ˜", 1, 10, 3)
    if st.button("RAG ê²€ìƒ‰ ì‹¤í–‰"):
        if not q.strip():
            st.warning("ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
        else:
            with st.spinner("ê²€ìƒ‰ ì¤‘..."):
                r = query_pdf_knowledge(q.strip(), n_results=nres)
            if not r:
                st.info("ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ì´ {len(r)}ê±´")
                # ê²°ê³¼ í˜•íƒœ(êµ¬ë²„ì „/ì‹ ë²„ì „) ëª¨ë‘ ëŒ€ì‘
                for i, item in enumerate(r, 1):
                    if isinstance(item, dict):
                        st.markdown(f"**[{i}] ì¶œì²˜: {item.get('source','unknown')}**")
                        st.write(item.get("content","")[:800] + "â€¦")
                    else:
                        st.markdown(f"**[{i}]**")
                        st.write(item[:800] + "â€¦")

# ---------------------------
# íƒ­ 3: í¬íŠ¸í´ë¦¬ì˜¤
# ---------------------------
with tab_port:
    st.subheader("ë‚´ í¬íŠ¸í´ë¦¬ì˜¤ ì‹œì„¸")
    tickers = [t.strip() for t in user_input.split(",") if t.strip()]
    pagent = PortfolioAgent(tickers=tickers)
    with st.spinner("ì‹œì„¸ ì¡°íšŒ ì¤‘..."):
        prices = pagent.get_prices()
    if not prices:
        st.info("í¬íŠ¸í´ë¦¬ì˜¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")
    else:
        rows = []
        for t, info in prices.items():
            ch = info.get("change")
            rows.append({
                "í‹°ì»¤": t,
                "ê°€ê²©": info.get("price"),
                "ë³€í™”ìœ¨(%)": None if ch is None else round(float(ch), 2),
                "ë¹„ê³ ": info.get("note") or info.get("error")
            })
        st.dataframe(rows, use_container_width=True)

    # ë‰´ìŠ¤ì™€ ì—°ë™ (í˜„ì¬ ì„¸ì…˜ ë¶„ì„ ê²°ê³¼ ê¸°ì¤€)
    st.markdown("### ë‰´ìŠ¤ì™€ ì¢…ëª© ë§¤ì¹­")
    if not st.session_state.analyzed:
        st.info("ìƒë‹¨ 'ë‰´ìŠ¤/ë¶„ì„' íƒ­ì—ì„œ ë¶„ì„ì„ ë¨¼ì € ì‹¤í–‰í•˜ë©´, ê´€ë ¨ ì¢…ëª© ë§¤ì¹­ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.")
    else:
        linked_rows = []
        for item in st.session_state.analyzed:
            art = item["article"]
            txt = (art["title"] + " " + art["summary"]).lower()
            related = [t for t in tickers if t.lower() in txt]
            linked_rows.append({
                "ì œëª©": art["title"][:90] + ("â€¦" if len(art["title"])>90 else ""),
                "ë§í¬": art["link"],
                "ê´€ë ¨ì¢…ëª©": ", ".join(related) if related else "-"
            })
        st.table(linked_rows)

# ---------------------------
# íƒ­ 4: ìƒíƒœ/ì„¤ì •
# ---------------------------
with tab_status:
    st.subheader("ìƒíƒœ/ì§„ë‹¨")
    # PDF DB ìƒíƒœ
    try:
        cnt = collection.count()
    except Exception:
        cnt = "N/A"
    st.write(f"PDF RAG chunks: **{cnt}**")

    # í™˜ê²½ ë³€ìˆ˜/ëª¨ë¸
    from dotenv import load_dotenv
    load_dotenv()
    api_key = os.getenv("OPENAI_API_KEY", "")
    model = os.getenv("MODEL", "gpt-4o-mini")
    st.write(f"OpenAI API Key ì„¤ì •ë¨: **{'Yes' if api_key else 'No'}**")
    st.write(f"ëª¨ë¸: **{model}**")

    st.caption("â€» íˆ¬ì ìë¬¸ ì•„ë‹˜. ë°ì´í„°ëŠ” ì•¼í›„ íŒŒì´ë‚¸ìŠ¤ ë¬´ë£Œ ì†ŒìŠ¤(ì§€ì—° ê°€ëŠ¥). PDFëŠ” ë¡œì»¬ VectorDB(Chroma)ì— ì €ì¥ë˜ì–´ RAGë¡œ ê²€ìƒ‰ë©ë‹ˆë‹¤.")

with tab_signal:
    st.subheader("ğŸš€ ì˜¤ëŠ˜ì˜ ì£¼ëª© ì¢…ëª©")
    signals = st.session_state.get("signals", [])
    if not signals:
        st.info("ì•„ì§ ì‹œê·¸ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Orchestrator ì „ì²´ ì‹¤í–‰ í›„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        st.dataframe(signals)

with tab_signal:
    st.subheader("ğŸš€ ì˜¤ëŠ˜ì˜ ì£¼ëª© ì¢…ëª©")
    signals = st.session_state.get("signals", [])
    if not signals:
        st.info("ì•„ì§ ì‹œê·¸ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. Orchestrator ì „ì²´ ì‹¤í–‰ í›„ í™•ì¸í•˜ì„¸ìš”.")
    else:
        st.dataframe(signals)
