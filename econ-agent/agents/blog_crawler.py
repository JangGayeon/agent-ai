import feedparser
import datetime

class BlogCrawlerAgent:
    """
    ê²½ì œ/ì£¼ì‹ ê´€ë ¨ ë¸”ë¡œê·¸/ì¹¼ëŸ¼ RSSì—ì„œ ê¸€ ìˆ˜ì§‘
    """
    def __init__(self, horizon_days=3):
        self.horizon_days = horizon_days
        self.sources = [
            # ğŸ”½ ì—¬ê¸°ì— ì›í•˜ëŠ” ë¸”ë¡œê·¸/ì¹¼ëŸ¼ RSS URL ì¶”ê°€
            # ë„¤ì´ë²„ ë¸”ë¡œê·¸ëŠ” ì§ì ‘ RSSë¥¼ ë„£ê¸° ì–´ë µê³ , ê²½ì œ ì „ë¬¸ ì¹¼ëŸ¼/ë§¤ì²´ RSSë¥¼ ë„£ëŠ” ê²Œ ì•ˆì •ì 
            "https://seekingalpha.com/market_currents.xml",  # ì‹œí‚¹ì•ŒíŒŒ
            "https://www.kiplinger.com/feeds/rss.xml",      # Kiplinger (ê²½ì œ/íˆ¬ì)
            "https://rss.nytimes.com/services/xml/rss/nyt/Economy.xml", # NYT ê²½ì œ ì„¹ì…˜
        ]

    def collect_items(self):
        """RSS ê¸°ë°˜ ë¸”ë¡œê·¸/ì¹¼ëŸ¼ ìˆ˜ì§‘"""
        now = datetime.datetime.utcnow()
        horizon = now - datetime.timedelta(days=self.horizon_days)
        results = []

        for url in self.sources:
            feed = feedparser.parse(url)
            for entry in feed.entries:
                # pubDate ê°€ì ¸ì˜¤ê¸°
                published = None
                if hasattr(entry, "published_parsed") and entry.published_parsed:
                    published = datetime.datetime(*entry.published_parsed[:6])
                else:
                    published = now

                if published < horizon:
                    continue

                item = {
                    "title": entry.get("title", ""),
                    "summary": entry.get("summary", ""),
                    "link": entry.get("link", ""),
                    "published": published.isoformat(),
                    "source": url
                }
                results.append(item)
        return results


if __name__ == "__main__":
    crawler = BlogCrawlerAgent(horizon_days=3)
    blogs = crawler.collect_items()
    print(f"ì´ {len(blogs)}ê°œ ë¸”ë¡œê·¸/ì¹¼ëŸ¼ ìˆ˜ì§‘ë¨")
    for b in blogs[:5]:
        print(f"- {b['title']} ({b['published']})\n  {b['link']}")
