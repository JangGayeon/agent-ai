import re
import hashlib
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import feedparser

# ì„¤ì •
KST = ZoneInfo("Asia/Seoul")
FEEDS = [
    "https://feeds.reuters.com/reuters/businessNews",
    "https://feeds.a.dj.com/rss/RSSMarketsMain.xml",  # WSJ Markets
    "https://www.ft.com/rss/home/asia",               # FT Asia
    "https://news.google.com/rss/search?q=%EA%B2%BD%EC%A0%9C&hl=ko&gl=KR&ceid=KR:ko"
]

# ì¤‘ìš” í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ìš©)
KEYWORDS = [
    "ê¸ˆë¦¬", "ë¬¼ê°€", "í™˜ìœ¨", "ì‹¤ì ", "ìˆ˜ì¶œ", "ë°˜ë„ì²´", "ì›ìœ ", "AI",
    "ê³ ìš©", "ì¤‘êµ­", "ë¯¸êµ­", "ECB", "FOMC", "CPI", "PPI", "GDP", "ì—°ì¤€"
]

class NewsCrawlerAgent:
    def __init__(self, horizon_hours=18):
        """
        horizon_hours: ìµœê·¼ ëª‡ ì‹œê°„ ì´ë‚´ì˜ ë‰´ìŠ¤ë¥¼ ê°€ì ¸ì˜¬ì§€ (ê¸°ë³¸ 18ì‹œê°„)
        """
        self.horizon_hours = horizon_hours

    def collect_items(self):
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ê¸°ì‚¬ ìˆ˜ì§‘"""
        since = datetime.now(KST) - timedelta(hours=self.horizon_hours)
        items = []
        for url in FEEDS:
            feed = feedparser.parse(url)
            for e in feed.entries:
                # ë°œí–‰ ì‹œê°„
                if hasattr(e, "published_parsed") and e.published_parsed:
                    pub = datetime(*e.published_parsed[:6], tzinfo=ZoneInfo("UTC")).astimezone(KST)
                else:
                    pub = datetime.now(KST)

                if pub < since:
                    continue

                title = e.title.strip()
                link = getattr(e, "link", "").strip()
                summary = re.sub(r"<[^>]+>", "", getattr(e, "summary", "")).strip()

                items.append({
                    "title": title,
                    "link": link,
                    "summary": summary,
                    "published": pub,
                    "source": url
                })
        return items

    def rank_items(self, items, topk=10):
        """í‚¤ì›Œë“œ ë§¤ì¹­ + ìµœì‹ ì„± ê¸°ë°˜ ì ìˆ˜ë¡œ ì •ë ¬"""
        seen = set()
        scored = []

        for it in items:
            # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
            key = hashlib.md5(it["title"].lower().encode()).hexdigest()
            if key in seen:
                continue
            seen.add(key)

            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
            txt = (it["title"] + " " + it["summary"]).lower()
            kw_score = sum(1 for kw in KEYWORDS if kw.lower() in txt) * 1.0

            # ìµœì‹ ì„± ê°€ì¤‘ì¹˜ (ìµœê·¼ì¼ìˆ˜ë¡ ë†’ìŒ)
            hours_ago = (datetime.now(KST) - it["published"]).total_seconds() / 3600
            recency = max(0, 24 - hours_ago) * 0.3

            score = kw_score + recency
            scored.append((score, it))

        # ì ìˆ˜ìˆœ ì •ë ¬
        scored.sort(key=lambda x: x[0], reverse=True)
        return scored[:topk]


# ì§ì ‘ ì‹¤í–‰í•  ë•Œ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    crawler = NewsCrawlerAgent()
    articles = crawler.collect_items()
    print(f"ì´ {len(articles)}ê°œ ê¸°ì‚¬ í¬ë¡¤ë§ë¨")

    ranked = crawler.rank_items(articles, topk=10)
    print("\nğŸ“Œ ìƒìœ„ 10ê°œ ê¸°ì‚¬:")
    for i, (score, art) in enumerate(ranked, 1):
        print(f"\n[{i}] ì ìˆ˜: {round(score,2)}")
        print("ì œëª©:", art["title"])
        print("ë§í¬:", art["link"])
        print("ì‹œê°„:", art["published"])
        print("ìš”ì•½:", art["summary"][:120], "...")
